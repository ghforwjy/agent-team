# 模块一详细设计计划

## 相关资源位置

### 核心代码
```
knowledge-work-plugins/it-audit/skills/1-audit-item-collector/scripts/
├── cleaner.py              # 清洗流程主程序
├── semantic_matcher.py     # 语义匹配模块
├── llm_verifier.py         # LLM校验模块
├── excel_parser.py         # Excel解析器
└── db_manager.py           # 数据库管理
```

### 数据分析工具
```
audit-analysis-tool/
├── main.py                 # 分析工具入口
├── src/
│   ├── data_loader.py     # 数据加载
│   ├── analyzer.py        # 分析逻辑
│   ├── reporter.py        # 报告生成
│   └── cli.py             # 命令行接口
└── output/                # 分析报告输出目录
```

**运行分析工具：**
```bash
# 生成HTML报告
python audit-analysis-tool/main.py --db-path tests/test_data/test_it_audit.db --group-by-item --output audit-analysis-tool/output/report.html

# 命令行查看统计
python audit-analysis-tool/main.py --db-path tests/test_data/test_it_audit.db --group-by-item
```

---

## 一、核心概念澄清

### 1.1 数据模型调整

**原设计问题**：
- 审计项(audit_items)中包含了audit_procedure字段
- 一个审计项只能有一个审计程序
- 无法积累多个来源的不同审计动作

**新设计**：
```
审计项 (audit_items)
    │
    └─── 一对多 ───▶ 审计动作 (audit_procedures)
                         │
                         └─── 每个动作来自不同来源
```

### 1.2 清洗逻辑

```
导入新底稿
    │
    ▼
逐条处理审计记录
    │
    ▼
语义匹配：是否已存在相同审计项？
    │
    ├─ 不存在 ──▶ 创建新审计项 + 创建新审计动作
    │
    └─ 已存在 ──▶ 语义匹配：是否已存在相同审计动作？
                      │
                      ├─ 不存在 ──▶ 新增审计动作到该审计项
                      │
                      └─ 已存在 ──▶ 跳过（记录来源）
```

## 二、数据库表结构修改

### 2.1 新增表：audit_procedures（审计动作表）

```sql
CREATE TABLE audit_procedures (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    item_id INTEGER NOT NULL,              -- 关联审计项
    procedure_text TEXT NOT NULL,          -- 审计程序/检查方法
    procedure_type VARCHAR(50),            -- 动作类型：查阅/访谈/测试/观察
    source_id INTEGER,                     -- 关联来源记录
    similarity_vector BLOB,                -- 语义向量（用于相似度匹配）
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (item_id) REFERENCES audit_items(id),
    FOREIGN KEY (source_id) REFERENCES audit_item_sources(id)
);
```

### 2.2 修改表：audit_items

- 移除 audit_procedure 字段
- 保留 title 作为核心匹配字段

### 2.3 修改表：audit_item_sources

- 关联到 audit_procedures 而非 audit_items
- 或保持不变，通过 audit_procedures.source_id 关联

## 三、清洗流程详细设计

### 3.1 整体流程

```
┌─────────────────────────────────────────────────────────────────┐
│ 步骤1: Excel解析                                                 │
│   - 读取Excel文件                                                │
│   - 智能识别表头位置                                             │
│   - 列名映射到标准字段                                           │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤2: 数据提取                                                  │
│   - 提取：维度、审计项标题、审计程序                             │
│   - 过滤：检查结论、检查记录、证据清单（属于审计结果）           │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤3: 审计项语义匹配（向量模型）                                │
│   - 使用向量模型计算语义相似度                                   │
│   - 阈值判断：                                                   │
│     > 0.85: 高相似，进入步骤4A                                   │
│     0.60-0.85: 中相似，进入步骤4B                               │
│     < 0.60: 低相似，视为新审计项，进入步骤6                      │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤4A: LLM校验（高相似情况）                                    │
│   - 调用LLM判断是否真的是同一审计项                              │
│   - 判断结果：                                                   │
│     确认合并 → 进入步骤5                                         │
│     应分离 → 视为新审计项，进入步骤6                             │
│     低置信度 → 标记待人工确认                                    │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤4B: 人工确认（中相似情况）                                   │
│   - 展示相似审计项供用户选择                                     │
│   - 用户选择：合并 / 分离                                        │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤5: 审计动作匹配（审计项已存在时）                            │
│   - 对比新审计程序与已有审计程序                                 │
│   - 语义相似度判断：                                             │
│     > 0.80: 调用LLM校验是否同一动作                              │
│     < 0.80: 新增审计动作                                         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│ 步骤6: 入库                                                      │
│   - 新审计项：写入audit_items                                    │
│   - 新审计动作：写入audit_procedures                             │
│   - 来源记录：写入audit_item_sources                             │
│   - 校验记录：写入校验日志表                                     │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 语义匹配详细设计

#### 3.2.1 审计项匹配

```python
def match_audit_item(new_title: str, existing_items: List[dict]) -> MatchResult:
    """
    匹配审计项
    
    Args:
        new_title: 新导入的审计项标题
        existing_items: 数据库中已有的审计项列表
    
    Returns:
        MatchResult: 匹配结果
            - match_type: 'exact' | 'high_similar' | 'medium_similar' | 'new'
            - matched_item: 匹配到的审计项（如有）
            - similarity_score: 相似度分数
    """
    # 1. 完全匹配检查
    for item in existing_items:
        if normalize(new_title) == normalize(item['title']):
            return MatchResult('exact', item, 1.0)
    
    # 2. 语义相似度计算
    new_vector = model.encode(new_title)
    
    best_match = None
    best_score = 0
    
    for item in existing_items:
        existing_vector = get_cached_vector(item['id']) or model.encode(item['title'])
        score = cosine_similarity(new_vector, existing_vector)
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # 3. 阈值判断
    if best_score > 0.85:
        return MatchResult('high_similar', best_match, best_score)
    elif best_score > 0.60:
        return MatchResult('medium_similar', best_match, best_score)
    else:
        return MatchResult('new', None, best_score)
```

#### 3.2.2 审计动作匹配

```python
def match_audit_procedure(new_procedure: str, existing_procedures: List[dict]) -> MatchResult:
    """
    匹配审计动作
    
    逻辑与审计项匹配类似，但阈值可能不同
    """
    # 审计动作的相似度阈值可以更宽松
    # 因为同一审计项的不同动作可能有细微差异
    ...
```

### 3.3 性能优化设计

> **核心问题**: 200条新 × 600条已有 = 120,000次比较，如果每次都调用LLM，成本和时间不可接受。

#### 3.3.1 两阶段匹配策略

```
阶段1: 向量模型初筛（快速、低成本）
┌─────────────────────────────────────────────────────────────────┐
│ 输入: 200条新审计项 + 600条已有审计项                            │
│ 处理: 批量计算向量，矩阵乘法求相似度                             │
│ 输出: 每条新审计项的Top-K候选（K=3）                             │
│ 耗时: 约10秒（本地模型）                                         │
│ 成本: 0元                                                        │
└─────────────────────────────────────────────────────────────────┘
                             │
                             ▼
阶段2: LLM校验（仅对候选结果）
┌─────────────────────────────────────────────────────────────────┐
│ 输入: 约20-50条需要校验的候选对                                  │
│ 处理: 批量LLM校验（一次调用处理多个）                            │
│ 输出: 最终合并决策                                               │
│ 耗时: 约30秒                                                     │
│ 成本: 约0.1-0.5元                                                │
└─────────────────────────────────────────────────────────────────┘
```

#### 3.3.2 向量模型批量匹配

```python
import numpy as np
from sentence_transformers import SentenceTransformer

def batch_match_items(new_titles: List[str], existing_items: List[dict], top_k: int = 3):
    """
    批量匹配审计项（向量模型）
    性能: 200条 × 600条 ≈ 10秒（本地模型）
    """
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    # 批量计算新审计项向量
    new_vectors = model.encode(new_titles, batch_size=32)
    
    # 获取已有审计项向量（优先使用缓存）
    existing_vectors = np.array([
        item.get('title_vector') or model.encode(item['title'])
        for item in existing_items
    ])
    
    # 矩阵乘法计算相似度（一次计算所有配对）
    similarity_matrix = np.dot(new_vectors, existing_vectors.T)
    
    # 为每条新审计项找到Top-K候选
    results = {}
    for i, new_title in enumerate(new_titles):
        top_indices = np.argsort(similarity_matrix[i])[-top_k:][::-1]
        results[new_title] = [
            {'item': existing_items[idx], 'similarity': float(similarity_matrix[i][idx])}
            for idx in top_indices
            if similarity_matrix[i][idx] > 0.60
        ]
    
    return results
```

#### 3.3.3 LLM批量校验

```python
def batch_llm_verify(candidates: List[Dict], batch_size: int = 10):
    """
    批量LLM校验（减少API调用次数）
    将多个校验任务合并到一个prompt中
    """
    results = []
    for i in range(0, len(candidates), batch_size):
        batch = candidates[i:i+batch_size]
        prompt = f"请判断以下{len(batch)}对审计项是否应该合并..."
        response = llm.call(prompt)
        results.extend(parse_response(response))
    return results
```

#### 3.3.4 调用次数估算

| 场景 | 向量比较次数 | LLM调用次数 | 成本 |
|------|-------------|-------------|------|
| 200条新，600条已有 | 120,000次 | 约5-10次 | 约0.1-0.5元 |
| 预计高相似匹配 | - | 约20-50条 | 本地向量模型免费 |

### 3.4 LLM校验设计

> **设计目的**: 向量模型只能计算语义相似度，无法进行逻辑判断。需要LLM对清洗结果进行二次校验，避免错误合并。

#### 3.3.1 校验时机

在语义匹配完成后、入库之前，对以下情况进行LLM校验：

| 场景 | 触发条件 | 校验目的 |
|------|----------|----------|
| 审计项合并校验 | 语义相似度 > 0.85 时 | 确认是否真的是同一检查项 |
| 审计动作合并校验 | 语义相似度 > 0.80 时 | 确认检查方法是否真的相同 |
| 维度一致性校验 | 新增审计项时 | 确认维度分类是否合理 |

#### 3.3.2 校验Prompt模板

```python
LLM_VERIFY_PROMPT = """
你是一个IT审计专家，负责审核审计项的合并决策是否正确。

## 待审核内容

**已有审计项**:
- 标题: {existing_title}
- 维度: {existing_dimension}
- 审计动作: {existing_procedures}

**新导入审计项**:
- 标题: {new_title}
- 维度: {new_dimension}
- 审计程序: {new_procedure}
- 语义相似度: {similarity_score}

## 审核要点

请判断以下问题：

1. **是否同一审计项？**
   - 检查重点是否相同？
   - 检查对象是否相同？
   - 是否存在包含关系（一个是另一个的子项）？

2. **维度是否一致？**
   - 如果维度不同，是否合理？
   - 是否应该调整维度？

3. **审计动作是否相同？**
   - 检查方法是否相同？
   - 检查对象是否相同？
   - 是否需要保留为不同动作？

## 输出格式

请以JSON格式输出审核结果：
{
    "is_same_item": true/false,
    "item_merge_decision": "merge"/"keep_separate",
    "item_reason": "判断理由",
    "is_same_procedure": true/false,
    "procedure_merge_decision": "merge"/"keep_separate", 
    "procedure_reason": "判断理由",
    "dimension_adjustment": null 或建议的维度,
    "confidence": "high"/"medium"/"low"
}
"""
```

#### 3.3.3 典型错误案例

```
案例1: 假阳性合并
- 审计项A: "是否建立IT治理委员会"
- 审计项B: "IT治理委员会是否有效运作"
- 语义相似度: 0.88 (高相似)
- LLM判断: 不是同一审计项
  - A检查的是"有没有建立"
  - B检查的是"运作是否有效"
  - 检查重点不同，应保持分离

案例2: 动作误合并
- 动作A: "查阅IT治理委员会成立发文"
- 动作B: "查阅IT治理委员会会议记录"
- 语义相似度: 0.82 (高相似)
- LLM判断: 不是同一动作
  - A检查的是"成立文件"
  - B检查的是"会议记录"
  - 检查对象不同，应保留两个动作

案例3: 维度错配
- 审计项: "是否制定网络安全应急预案"
- 自动归类维度: "IT运维"
- LLM判断: 应归入"事件与应急管理"
```

### 3.4 冲突处理策略

| 场景 | 相似度 | 处理方式 | 说明 |
|------|--------|----------|------|
| 完全相同审计项 | 1.0 | 合并 | 直接关联到已有审计项 |
| 高相似审计项 | >0.85 | 自动合并 | 关联到最相似的审计项 |
| 中相似审计项 | 0.60-0.85 | 人工确认 | 提示用户选择合并或新建 |
| 新审计项 | <0.60 | 新建 | 创建新审计项 |
| 审计动作相同 | >0.80 | 跳过 | 只记录来源 |
| 审计动作不同 | <0.80 | 新增 | 添加新动作 |

## 四、向量模型使用

### 4.1 模型选择

模型名称：`paraphrase-multilingual-MiniLM-L12-v2`

本地路径：`model/Sentence-BERT/`

特点：
- 多语言支持（中文友好）
- 向量维度：384
- 适合语义相似度计算
- 本地加载，无需联网

### 4.2 向量缓存策略

```python
# 在 audit_items 表中添加向量字段
ALTER TABLE audit_items ADD COLUMN title_vector BLOB;

# 或创建单独的向量缓存表
CREATE TABLE item_vectors (
    item_id INTEGER PRIMARY KEY,
    title_vector BLOB,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (item_id) REFERENCES audit_items(id)
);
```

### 4.3 批量向量化

```python
def batch_encode_items(items: List[str], batch_size: int = 32) -> List[np.array]:
    """批量计算向量，提高效率"""
    vectors = []
    for i in range(0, len(items), batch_size):
        batch = items[i:i+batch_size]
        batch_vectors = model.encode(batch)
        vectors.extend(batch_vectors)
    return vectors
```

## 五、实施步骤

### 5.1 数据库修改
1. 创建 audit_procedures 表
2. 从 audit_items 迁移现有 audit_procedure 数据
3. 移除 audit_items.audit_procedure 字段
4. 添加向量缓存字段/表

### 5.2 代码修改
1. 修改 db_manager.py - 支持新表结构
2. 创建 semantic_matcher.py - 语义匹配模块
3. 修改 collector.py - 实现新的清洗逻辑
4. 更新 SKILL.md - 更新技能说明

### 5.3 测试验证
1. 使用现有数据测试语义匹配
2. 验证清洗逻辑正确性
3. 验证数据完整性

## 六、预期效果

导入多个底稿后，数据库结构：

```
audit_items (审计项)
├── "公司是否建立IT治理委员会"
│   ├── audit_procedures:
│   │   ├── "查阅IT治理委员会成立发文" (来源: 2021底稿)
│   │   └── "检查公司是否制定IT治理委员会成立文件" (来源: 2022底稿)
│   └── sources: [2021底稿-行6, 2022底稿-行3]
│
├── "是否制定网络安全保障方案"
│   ├── audit_procedures:
│   │   └── "检查公司是否制定专门的网络安全保障方案"
│   └── sources: [2021底稿-行3]
│
└── ...
```

这样逐步积累，形成一个完整的审计知识库。
